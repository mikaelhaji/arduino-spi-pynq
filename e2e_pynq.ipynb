{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.22621-SP0\n",
      "PyTorch Version: 2.0.1+cpu\n",
      "\n",
      "Python 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "Pandas 2.0.3\n",
      "GPU is NOT AVAILABLE\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is cpu\n"
     ]
    }
   ],
   "source": [
    "import snntorch as snn\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import pprint as pp\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "from snntorch import surrogate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init LIFNet and Netron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net78x78(nn.Module):\n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_steps = config[\"sequence_length\"]\n",
    "        self.input_dim = config[\"input_dim\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.device = device\n",
    "\n",
    "        self.spike_grads = {\n",
    "            \"fast_sigmoid\": surrogate.fast_sigmoid(),\n",
    "            \"arctan\": surrogate.atan(),\n",
    "            \"LSO\": surrogate.LSO(), \n",
    "        }\n",
    "\n",
    "        # Init layers\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.input_dim)\n",
    "        # Init fc1.weight with custom_weight\n",
    "        # self.fc1.weight.data = torch.nn.Parameter(torch.from_numpy(custom_weight).float())\n",
    "\n",
    "        self.lif1 = snn.Leaky(beta=config['beta'], spike_grad=self.spike_grads[config['surrogate']]) # learn beta to implement on Netron\n",
    "\n",
    "        self.fc2 = nn.Linear(self.input_dim, config['hid_layers'][0])\n",
    "        self.lif2 = snn.Leaky(beta=config['beta'], spike_grad=self.spike_grads[config['surrogate']])\n",
    "\n",
    "        self.lif_layers = nn.ModuleList()\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "\n",
    "        # Create hiden layers with LIF neurons\n",
    "        for i in range(len(config['hid_layers'])-1):\n",
    "            self.fc_layers.append(nn.Linear(config['hid_layers'][i], config['hid_layers'][i+1]))\n",
    "            self.lif_layers.append(snn.Leaky(beta=config['beta'], spike_grad=self.spike_grads[config['surrogate']]))\n",
    "\n",
    "        # Final layer\n",
    "        self.fc_final = nn.Linear(config['hid_layers'][-1], self.num_classes)\n",
    "        self.lif_final = snn.Leaky(beta=config['beta'], threshold=config['out_threshold'], spike_grad=self.spike_grads[config['surrogate']])\n",
    "\n",
    "        self.dropout = nn.Dropout(p=config[\"dropout\"])\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # Init hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem_layers = [lif_layer.init_leaky() for lif_layer in self.lif_layers]\n",
    "        mem_final = self.lif_final.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk_final_rec = []\n",
    "        mem_final_rec = []\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            cur = self.fc1(x[step])\n",
    "            spk, mem1 = self.lif1(cur, mem1)\n",
    "\n",
    "            cur = self.fc2(spk)\n",
    "            spk, mem2 = self.lif2(cur, mem2)\n",
    "\n",
    "            for i in range(len(self.fc_layers)):\n",
    "                cur = self.fc_layers[i](spk)\n",
    "                cur = self.dropout(cur)\n",
    "                spk, mem_layers[i] = self.lif_layers[i](cur, mem_layers[i])\n",
    "\n",
    "            cur = self.fc_final(spk)\n",
    "            cur = self.dropout(cur)\n",
    "            spk_final, mem_final = self.lif_final(cur, mem_final)\n",
    "\n",
    "            spk_final_rec.append(spk_final)\n",
    "            mem_final_rec.append(mem_final)\n",
    "\n",
    "        return torch.stack(spk_final_rec, dim=0), torch.stack(mem_final_rec, dim=0)\n",
    "\n",
    "class Netron():\n",
    "    def __init__(self, config, device):\n",
    "        self.model_path = config['model_path']\n",
    "        self.edm_thr_path = config['EDM_thr_path']\n",
    "        self.device = device\n",
    "        self.load_model()\n",
    "        self.load_EDM_thr()\n",
    "\n",
    "    def load_model(self):\n",
    "        checkpoint = torch.load(self.model_path, map_location=self.device)\n",
    "        self.model_config = checkpoint['config']\n",
    "        self.model = Net78x78(self.model_config, self.device)\n",
    "        self.state_dict = checkpoint['model_state']\n",
    "        self.model.load_state_dict(self.state_dict, strict=False)\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_EDM_thr(self):\n",
    "        with open (self.edm_thr_path, 'rb') as f:\n",
    "            self.ch_ref = pickle.load(f)\n",
    "            self.edm_thr = {k: v['edm_thresholds'] for k, v in self.ch_ref.items()}\n",
    "\n",
    "    def forward_pass_e2e(self, input, *args, **kwargs):\n",
    "        edm_spiketrains = self.forward_pass_EDM(input, *args, **kwargs)\n",
    "        spk_rec, mem_rec = self.forward_pass_SNN(edm_spiketrains)\n",
    "        return spk_rec, mem_rec, edm_spiketrains\n",
    "\n",
    "    def forward_pass_EDM(self, input, drop_ch=None):\n",
    "        edm_output = self.edm_vec(input)  # (n_alpha, n_samples, n_channels)\n",
    "        edm_output = np.moveaxis(edm_output, [2, 0], [0, 1])  # (n_channels, n_alpha, n_samples)\n",
    "        edm_spiketrains = np.zeros(input.shape)\n",
    "\n",
    "        for i, ch in enumerate(self.edm_thr):\n",
    "            condition1 = (self.edm_thr[ch][0] < edm_output[i, 0]) & (edm_output[i, 0] < self.edm_thr[ch][1]) # 1 - 4 comparators \n",
    "            condition2 = (self.edm_thr[ch][2] < edm_output[i, 1]) & (edm_output[i, 1] < self.edm_thr[ch][3])\n",
    "            st = condition1 & condition2 # final andgate \n",
    "            # use a nand gate instead\n",
    "            # st = ~(condition1 & condition2)\n",
    "            edm_spiketrains[st, i] = 1 \n",
    "        \n",
    "        if drop_ch:\n",
    "            edm_spiketrains = np.delete(edm_spiketrains, list(map(netron.idx_translate, drop_ch)), axis=1)\n",
    "\n",
    "        return edm_spiketrains\n",
    "                               \n",
    "    def forward_pass_SNN(self, input):\n",
    "        # input to this function will be the output of forward_pass_EDM\n",
    "        # convert to tensor and unsqueeze to add batch dimension\n",
    "        input = torch.from_numpy(input).unsqueeze(0).float().permute(1, 0, 2) # (n_samples, batch_size, n_channels)\n",
    "        spk_rec, mem_rec = self.model.forward(input)\n",
    "\n",
    "        return spk_rec, mem_rec \n",
    "\n",
    "    def edm_vec(self, x, alphas=[1, 3], init_edm=0.5):\n",
    "        n_alpha = len(alphas)\n",
    "        edm_output = np.zeros((n_alpha, x.shape[0], x.shape[1])) # (n_alpha, n_samples, n_channels)\n",
    "        edm = np.ones((n_alpha, x.shape[1])) * init_edm\n",
    "        alphas = np.array([1/2**a for a in alphas]).reshape(-1, 1)\n",
    "\n",
    "        for t in range(x.shape[0]):\n",
    "            edm = edm - alphas * (edm - x[t, :].reshape(1, -1))\n",
    "            edm_output[:, t, :] = edm\n",
    "        \n",
    "        return edm_output\n",
    "    \n",
    "    def idx_translate(self, ch_id):\n",
    "       return list(self.ch_ref.keys()).index(ch_id)\n",
    "    \n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load('C:/Users/mikae/Downloads/netron inference-20230812T013749Z-001/netron inference/dataset/ap_dset.npz')\n",
    "X, Y, markers = loaded['X'], loaded['Y'], loaded['markers']\n",
    "\n",
    "loaded_test = np.load('C:/Users/mikae/Downloads/netron inference-20230812T013749Z-001/netron inference/dataset/test_set_ap.npz')\n",
    "X_test, Y_test = loaded_test['X'], loaded_test['Y'], \n",
    "\n",
    "markers = markers.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "netron_config = {\n",
    "    'model_path': 'C:/Users/mikae/Downloads/netron inference-20230812T013749Z-001/netron inference/models/final_ngc536dn.pth',\n",
    "    'EDM_thr_path': 'C:/Users/mikae/Downloads/netron inference-20230812T013749Z-001/netron inference/models/EDMNet_liberal_thresholds_edm1driven.pkl', \n",
    "}\n",
    "\n",
    "netron = Netron(netron_config, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_shape': (142, 3000, 73),\n",
      " 'aug_dynamic': None,\n",
      " 'aug_prob': 0.4,\n",
      " 'aug_strategy': 'single',\n",
      " 'aug_trials': 1000,\n",
      " 'augmented': None,\n",
      " 'batch_size': 8,\n",
      " 'beta': 0.9,\n",
      " 'classes': {'PG': 0, 'SG': 1},\n",
      " 'dropout': 0.0,\n",
      " 'dset': 'edm1driven_dropped',\n",
      " 'enable_wandb': True,\n",
      " 'grad_clip': None,\n",
      " 'grad_mask': True,\n",
      " 'hid_layers': [256],\n",
      " 'input_dim': 73,\n",
      " 'loss': 'CECountLoss',\n",
      " 'lr': 0.0005,\n",
      " 'lr_scheduler': 'none',\n",
      " 'n_electrodes': 73,\n",
      " 'name': 'edm1driven_2152',\n",
      " 'num_classes': 2,\n",
      " 'num_epochs': 200,\n",
      " 'optimizer': 'Adam',\n",
      " 'out_threshold': 1.0,\n",
      " 'overfit_minibatch': False,\n",
      " 'project': 'lif_params',\n",
      " 'save_model_threshold': 0.0,\n",
      " 'sequence_length': 3000,\n",
      " 'surrogate': 'arctan',\n",
      " 'test_dataset_size': 15,\n",
      " 'test_minibatch_size': 2,\n",
      " 'test_split': 0.1,\n",
      " 'train_dataset_size': 127,\n",
      " 'train_minibatch_size': 16,\n",
      " 'unique': True,\n",
      " 'weight_decay': 0,\n",
      " 'weight_init': 'even_78x78',\n",
      " 'workers': 0}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(netron.model_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate EDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 78)\n",
      "(2, 3000, 78)\n"
     ]
    }
   ],
   "source": [
    "trial = 10\n",
    "sample_X = X[trial]\n",
    "sample_Y = Y[trial]\n",
    "print(sample_X.shape)\n",
    "edm_X = netron.edm_vec(sample_X)\n",
    "print(edm_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 73)\n",
      "324.0\n"
     ]
    }
   ],
   "source": [
    "drop_ch = [11, 33, 26, 58, 18]\n",
    "edm_spiketrain_X = netron.forward_pass_EDM(sample_X, drop_ch=drop_ch)\n",
    "print(edm_spiketrain_X.shape)\n",
    "print(np.sum(edm_spiketrain_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(edm_spiketrain_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test End2End Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike recording:  torch.Size([3000, 1, 2])\n",
      "Membrane potential recording:  torch.Size([3000, 1, 2])\n",
      "EDM spike trains:  (3000, 73)\n"
     ]
    }
   ],
   "source": [
    "spk_rec, mem_rec, edm_spiketrains = netron.forward_pass_e2e(sample_X, drop_ch=drop_ch)\n",
    "\n",
    "print(\"Spike recording: \", spk_rec.shape)\n",
    "print(\"Membrane potential recording: \", mem_rec.shape)\n",
    "print(\"EDM spike trains: \", edm_spiketrains.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arduino > PYNQ Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export EDM Spike Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to edm_spiketrains_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy array to pandas DataFrame\n",
    "df_edm_spiketrains = pd.DataFrame(edm_spiketrains)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "csv_filename = \"edm_spiketrains_data.csv\"\n",
    "df_edm_spiketrains.to_csv(csv_filename, index=False)\n",
    "print(f\"Data saved to {csv_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ismltorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
